---
description: Create multi-dimensional quality scoring rules.
---

# Judge Rules

Judge rules use weighted criteria and a 1-4 rubric for sophisticated quality measurement. They're ideal for assessing content quality on a spectrum.

## How Judge Rules Work

1. **LLM evaluates content** against multiple criteria
2. **Score Calculation**: Each criterion gets a 1-4 rating, normalized to 1-10 scale
3. **Final Score**: Weighted average of all criteria scores

### The 1-4 Scoring Scale

VectorLint uses a **1-4 scale** for all judge criteria:

| LLM Rating | Meaning | Normalized Score |
|:-----------|:---------|:-----------------|
| **4** | Excellent | **10.0** |
| **3** | Good | **7.0** |
| **2** | Fair | **4.0** |
| **1** | Poor | **1.0** |

**Normalization formula:**
```javascript
1 + ((Rating - 1) / 3) * 9
```

## Score Calculation

### Example: Headline Evaluator

```yaml
criteria:
  - name: Value Communication
    id: ValueCommunication
    weight: 12
  - name: Curiosity Gap
    id: CuriosityGap
    weight: 2
```

**Ratings:**
- Value Communication: 3 (Good) → 7.0 normalized
- Curiosity Gap: 4 (Excellent) → 10.0 normalized

**Weighted Scores:**
- Value Communication: 7.0 × 12 = 84 points
- Curiosity Gap: 10.0 × 2 = 20 points

**Final Score:**
```
(84 + 20) / (12 + 2) = 104 / 14 = 7.43
```

## Basic Example

```markdown
---
specVersion: 1.0.0
evaluator: base
type: judge
id: HeadlineEvaluator
name: Headline Evaluator

severity: error
criteria:
  - name: Value Communication
    id: ValueCommunication
    weight: 10
  - name: Language Authenticity
    id: LanguageAuthenticity
    weight: 5
---

You are a headline evaluator. Assess the H1 headline for:

1. **Value Communication** (10 points): Does it clearly state what the reader gains?
2. **Language Authenticity** (5 points): Does it use natural, conversational language?

For each criterion, provide:
- A score from 1-4
- Specific examples from the headline supporting your score
- Suggestions for improvement if below 4

## RUBRIC

# Value Communication <weight=10>

### Excellent <score=4>

Specific, immediately appealing benefit clearly stated.

### Good <score=3>

Clear benefit but less specific impact.

### Fair <score=2>

Generic or vague benefit.

### Poor <score=1>

No benefit communicated.

# Language Authenticity <weight=5>

### Excellent <score=4>

Natural, conversational tone. No buzzwords or clichés.

### Good <score=3>

Mostly natural with minor marketing language.

### Fair <score=2>

Noticeable marketing jargon or buzzwords.

### Poor <score=1>

Heavy marketing language, buzzwords, or clichés.
```

### Output

```bash
Headline Evaluator      ███████░░░ 7.5/10
✓ Value Communication: 3/4 (Good)
  → "Build APIs Faster" - Clear benefit but could be more specific
✓ Language Authenticity: 4/4 (Excellent)
  → Natural, conversational tone
```

## Criteria Structure

Each criterion has these properties:

| Property | Type | Required | Description |
|-----------|------|-----------|-------------|
| `name` | string | **Yes** | Human-readable criterion name |
| `id` | string | **Yes** | Unique identifier (PascalCase recommended) |
| `weight` | number | No | Importance weight (default: 1) |
| `target` | object | No | Criterion-specific content matching |

## Example: Content Quality Evaluator

```markdown
---
specVersion: 1.0.0
evaluator: base
type: judge
id: ContentQuality
name: Content Quality Evaluator

severity: warning
criteria:
  - name: Clarity
    id: Clarity
    weight: 30
  - name: Engagement
    id: Engagement
    weight: 25
  - name: Completeness
    id: Completeness
    weight: 25
  - name: Accuracy
    id: Accuracy
    weight: 20
---

Assess this content for quality across four dimensions.

## RUBRIC

# Clarity <weight=30>

### Excellent <score=4>

Content is immediately understandable. No ambiguity. Clear structure.

### Good <score=3>

Mostly clear with minor areas that could be improved.

### Fair <score=2>

Some unclear passages or confusing explanations.

### Poor <score=1>

Frequently unclear or confusing.

# Engagement <weight=25>

### Excellent <score=4>

Highly engaging from start to finish. Compelling examples.

### Good <score=3>

Engaging with room for improvement.

### Fair <score=2>

Somewhat engaging but not compelling.

### Poor <score=1>

Boring or difficult to follow.

# Completeness <weight=25>

### Excellent <score=4>

Comprehensive coverage. No obvious gaps.

### Good <score=3>

Good coverage with minor gaps.

### Fair <score=2>

Several gaps in coverage.

### Poor <score=1>

Significant gaps or incomplete.

# Accuracy <weight=20>

### Excellent <score=4>

All information appears accurate and up-to-date.

### Good <score=3>

Mostly accurate with minor issues.

### Fair <score=2>

Some inaccuracies or outdated information.

### Poor <score=1>

Multiple inaccuracies or significantly outdated.
```

## When to Use Judge Rules

Use judge rules when:

- You're measuring quality **on a spectrum** (e.g., "How engaging is this?")
- You have **multiple dimensions** (Clarity, Tone, Depth)
- You need **weighted importance** (some criteria matter more)
- Quality is **subjective** and nuanced

### Ideal Use Cases

| Use Case | Example Criteria |
|-----------|-----------------|
| **Content Quality** | Clarity, Engagement, Accuracy |
| **Headline Evaluation** | Value Communication, Curiosity Gap |
| **Tone & Voice** | Friendliness, Professionalism, Brand Alignment |
| **Technical Accuracy** | Correctness, Completeness, Currency |
| **Readability** | Sentence Length, Vocabulary, Structure |

## Using Weights Effectively

### Weights Reflect Importance

Scale weights to reflect real-world importance:

```yaml
criteria:
  # Critical for documentation
  - name: Technical Accuracy
    weight: 40

  # Important but not critical
  - name: Clarity
    weight: 30

  # Nice to have
  - name: Engagement
    weight: 20
```

### Weight Distribution Guidelines

| Scenario | Weight Range | Example |
|----------|--------------|---------|
| **Single dominant criterion** | One at 60+, others at 10-20 | SEO-focused (SEO = 70, Others = 15 each) |
| **Balanced evaluation** | All within 20-40 | General quality (30, 30, 25, 15) |
| **Many minor criteria** | All under 15 | Comprehensive checklist (10, 10, 10, 10, 10, 10, 10) |

## Rubric Best Practices

### 1. Define Clear Score Boundaries

Make the difference between 3 and 4 obvious:

❌ **Bad:**
```markdown
### Excellent
Good content.

### Good
Okay content.
```

✅ **Good:**
```markdown
### Excellent <score=4>
Specific, immediately appealing benefit. Reader knows exactly what they'll gain.

### Good <score=3>
Clear benefit but less specific impact or slightly vague.
```

### 2. Include Examples for Each Score

Provide examples that illustrate the difference:

```markdown
### Excellent <score=4>
**Examples:**
- "Build a REST API in 10 Minutes"
- "Reduce AWS Costs by 50%"

### Good <score=3>
**Examples:**
- "Build APIs Faster"
- "Save Money on Cloud Infrastructure"
```

### 3. Be Specific About What Lacks

For lower scores, explain what's missing:

```markdown
### Fair <score=2>
Generic or vague benefit. Reader unsure of outcome.
Missing: Specific metrics, actionable outcome, concrete benefit.
```

## Output Format

Judge rules produce output in this format:

```json
{
  "ruleId": "HeadlineEvaluator",
  "score": 7.5,
  "criteria": [
    {
      "id": "ValueCommunication",
      "name": "Value Communication",
      "score": 7.0,
      "rating": 3,
      "weight": 10
    },
    {
      "id": "LanguageAuthenticity",
      "name": "Language Authenticity",
      "score": 10.0,
      "rating": 4,
      "weight": 5
    }
  ]
}
```

## Next Steps

- **[Check Rules](./check-rules)** - Learn about pass/fail rules
- **[Rule Anatomy](./rule-anatomy)** - Rule structure overview
- **[Target Specification](./target-specification)** - Target specific content sections
